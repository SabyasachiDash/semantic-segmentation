
# Semantic Segmentation

In this project I trained a Fully Convolutional Network (FCN) to classify each pixel of an image as **ROAD** or **NOT ROAD**.

I used the **KITTI Dataset** avaialable at http://www.cvlibs.net/datasets/kitti/eval_road.php

The dataset consists of 289 training and 290 test images. It contains three different categories of road scenes:

- uu - urban unmarked (98/100)
- um - urban marked (95/96)
- umm - urban multiple marked lanes (96/94)
- urban - combination of the three above

Ground truth has been generated by manual annotation of the images and is available for two different road terrain types:

- road - the road area, i.e, the composition of all lanes, and
- lane - the ego-lane, i.e., the lane the vehicle is currently driving on (only available for category "um").

Ground truth is provided for training images only.

The original paper that made available the **KITTI Dataset** by _Jannik Fritsch et al._ can be found at http://www.cvlibs.net/publications/Fritsch2013ITSC.pdf

The FCN was based on the paper by _Jonathan Long et al._ https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf


## Predictions

##### Urban Multiple Marked Lanes

![png](images/output_5_0.png)

![png](images/output_5_1.png)

![png](images/output_5_2.png)


##### Urban Unmarked Lanes

![png](images/output_6_0.png)

![png](images/output_6_1.png)

![png](images/output_6_2.png)


##### Urban Marked Lanes

![png](images/output_7_0.png)

![png](images/output_7_1.png)

![png](images/output_7_2.png)


##### Misses

![png](images/output_8_0.png)

![png](images/output_8_1.png)

![png](images/output_8_2.png)


## Model

##### Architecture

Following the paper by Jonathan Long, it uses the original VGG 16 network and replaces the fully connected layers with three 1x1 convolutions for layers 7, 4 and 3, adding skip layers between them.

##### Parameters

- keep_prob: 0.5
- learning_rate: 0.0005
- epochs: 30
- batch_size: 8

After several trials, choosing a keep probability of 0.5, a learning rate of 0.0005 and 30 epochs in batches of 8 images was the run with good results. The loss continually decreased and in the 30th epoch it ended between 0.0200 and 0.0300.

```
  ...
  - loss   0.0242 (images: 8, labels: 8)
  - loss   0.0289 (images: 8, labels: 8)
  - loss   0.0181 (images: 1, labels: 1)
Running epoch 30/100
  ...
```

I run the final model for 100 epochs in batches of 8 images.
It took 50 minutes to complete (GTX 1080) and reached a final loss of about 0.0100

The final network generated the following TensorFlow model when saved:

```
SIZE   NAME
----------------------------------------
513M - model_01.pb
513M - model_01.meta
513M - model_01.ckpt.meta
4.8K - model_01.ckpt.index
1.6G - model_01.ckpt.data-00000-of-00001
```

##### Original

![png](images/output_22_0.png)

##### Resized

![png](images/output_23_0.png)

##### Softmax

![png](images/output_25_0.png)

##### Final Overlay

![png](images/output_26_0.png)


## Model Prediction on Test Images

A few examples from the best model run:

##### Urban Marked Lanes

<table>
<tr>
<td><img src="images/runs/um_000000.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000001.png" style="width: 350px;"></td>
<tr>
<td><img src="images/runs/um_000002.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000003.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000004.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000005.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000006.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000007.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000008.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000009.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000010.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000011.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000012.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000013.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000014.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000015.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000016.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000017.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/um_000018.png" style="width: 350px;"></td>
<td><img src="images/runs/um_000019.png" style="width: 350px;"></td>
</tr>
</table>

##### Urban Multiple Marked Lanes

<table>
<tr>
<td><img src="images/runs/umm_000000.png" style="width: 350px;"></td>
<td><img src="images/runs/umm_000001.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/umm_000002.png" style="width: 350px;"></td>
<td><img src="images/runs/umm_000003.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/umm_000004.png" style="width: 350px;"></td>
<td><img src="images/runs/umm_000005.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/umm_000006.png" style="width: 350px;"></td>
<td><img src="images/runs/umm_000007.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/umm_000008.png" style="width: 350px;"></td>
<td><img src="images/runs/umm_000009.png" style="width: 350px;"></td>
</tr>
</table>

##### Urban Unmarked Lanes

<table>
<tr>
<td><img src="images/runs/uu_000000.png" style="width: 350px;"></td>
<td><img src="images/runs/uu_000001.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/uu_000002.png" style="width: 350px;"></td>
<td><img src="images/runs/uu_000003.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/uu_000004.png" style="width: 350px;"></td>
<td><img src="images/runs/uu_000005.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/uu_000006.png" style="width: 350px;"></td>
<td><img src="images/runs/uu_000007.png" style="width: 350px;"></td>
</tr>
<tr>
<td><img src="images/runs/uu_000008.png" style="width: 350px;"></td>
<td><img src="images/runs/uu_000009.png" style="width: 350px;"></td>
</tr>
</table>

## Videos

I run the final model in some videos from my dashcam.
The results are remarkable good in portions of the route with similar characteristics than the KITTI dataset.

Considering that none of these images were used for training and the video was completely different, the predictions look good:

<a href="https://youtu.be/CezwSC9et3c?t=1m5s">
<img src="videos/route_A.png" style="width: 250px">
<a/>

<a href="https://youtu.be/ZEyXT-Qo2KU?t=1m">
<img src="videos/route_B.png" style="width: 250px">
<a/>

<a href="https://youtu.be/Kk2PtPU8hvI?t=1m40s">
<img src="videos/route_C.png" style="width: 250px">
<a/>


Complete videos:

- [Route A](https://youtu.be/CezwSC9et3c)
- [Route B](https://youtu.be/ZEyXT-Qo2KU)
- [Route C](https://youtu.be/Kk2PtPU8hvI)


## Notes

When building the initial model, I didn't consider the *kernel_initializer* parameter in the layers (it used the default initializer). That caused the model to generate segmentations with noisy borders:


<table>
<tr>
<th>kernel_initializer with default values</th>
<th>kernel_initializer with truncated normal values</th>
</tr>
<tr>
<td><img src="images/um_000013_without_kernel_initializer.png" style="width: 350px;"></td>
<td><img src="images/um_000013.png" style="width: 350px;"></td>
</tr>
</table>


## References

- [KITTI Dataset avaialable](http://www.cvlibs.net/datasets/kitti/eval_road.php)
- [Udacity's Semantic Segmentation Project](https://github.com/udacity/CarND-Semantic-Segmentation)
- [A quick complete tutorial to save and restore Tensorflow models](http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/)
